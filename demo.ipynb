{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Install and Import Modules\n",
    "#### Install \n",
    "Add more if needed. I have provided some installation commands but you may need more if you do not have them installed yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# %%capture is used to ignore output when content is boring.\n",
    "# This is how you install modules from jupyter notebook instead of terminal. \n",
    "# Install the one you need or comment them out \n",
    "!pip3 install -U imageio-ffmpeg             #It contains VideoFileClip module needed in the next cell. \n",
    "!pip3 install opencv-contrib-python==4.1.0  #It contains opencv module from https://www.pyimagesearch.com/2018/09/19/pip-install-opencv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import\n",
    "if errors then you may need to go back and install it first. (Restarting Kernel might be necessary too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "import ipywidgets as ipw\n",
    "import os\n",
    "import random\n",
    "from moviepy.editor import VideoFileClip,ImageSequenceClip\n",
    "from IPython.display import HTML, YouTubeVideo, display\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.1.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.__version__    # Make sure version above 3.4.3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Watch Input <font color=\"violet\"> Video </font>\n",
    "\n",
    "Run the sample video I have provided first along with the whole tutorial, then\n",
    "\n",
    "Try your own video:\n",
    "- Place your video in the `Input_Video` folder\n",
    "- Change <b>ONLY</b> the `video_name` in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <video width=\"960\" height=\"540\" controls>\n",
       "              <source src=\"Output_Video/sample_rotated.mp4\">\n",
       "            </video>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "video_name = \"sample.mp4\"\n",
    "input_video_path = os.path.join(\"Input_Video\", video_name)\n",
    "\n",
    "#Erase this!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "video_name_part = video_name.split(\".\")    # sample & mp4 has been split\n",
    "dest_path = os.path.join(\"Output_Video\", video_name_part[0] + \"_rotated.\" + video_name_part[1])\n",
    "input_video_path = dest_path\n",
    "\n",
    "# Watch Video\n",
    "display(HTML(\"\"\"\n",
    "            <video width=\"960\" height=\"540\" controls>\n",
    "              <source src=\"{0}\">\n",
    "            </video>\n",
    "            \"\"\".format(input_video_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Needs Rotation?\n",
    "I recorded my video with my phone in horizontal position. So I need to rotate it. Run the cells below if you also need rotation. Otherwise skip this section and go to next section: `Enumerate Frames of Video`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotateImage(image, angle):\n",
    "    \"\"\"\"\n",
    "    Performs a counter clockwise rotation of the angle specified. Set Angle to 90 to rotate left or -90 for right  \n",
    "    \"\"\"\n",
    "\n",
    "    (h, w) = image.shape[:2]        # get image height, width\n",
    "    cx, cy = int(w/2), int(h/2)     # calculate the center of the original image    \n",
    "\n",
    "    # Translate image center to the right before rotation\n",
    "    tx = cy-cx     #translate in x\n",
    "    ty = 0         #translate in y\n",
    "    M  = np.float32([[1,0,tx],[0,1,ty]])\n",
    "    image = cv2.warpAffine(image, M, (h,h))       # make square image (hxh)\n",
    "\n",
    "    # Perform the counter clockwise rotation holding at the center\n",
    "    center = (cy, cy)               # calculate new center of square image\n",
    "    scale  = 1.0                    # keep same dimension in image\n",
    "    M = cv2.getRotationMatrix2D(center, angle, scale)\n",
    "    image = cv2.warpAffine(image, M, (h, h))\n",
    "\n",
    "    # Translate image center to the top and crop bottom\n",
    "    tx = 0          #translate in x\n",
    "    ty = cx-cy      #translate in y\n",
    "    M  = np.float32([[1,0,tx],[0,1,ty]])\n",
    "    image = cv2.warpAffine(image, M, (h,w))       # this crops bottom as well\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement it on the first frame of the video to make sure everything looks okay. Set `angle` to 90 to rotate left or -90 for right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load video and Read frame\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 0) #First frame\n",
    "_, frame = cap.read() \n",
    "\n",
    "# Rotate to left\n",
    "angle = 90\n",
    "frame_rotated = rotateImage(frame , angle)\n",
    "\n",
    "# Display\n",
    "frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "frame_rotated = cv2.cvtColor(frame_rotated,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(11, 5))   #width, height in inches\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Original\")\n",
    "plt.imshow(frame)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Rotated\")\n",
    "plt.imshow(frame_rotated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works! So let's implemented in the whole video and place it in the `Output_Video` folder which we will create. Again set `angle` to 90 to rotate left or -90 for right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder to place output videos at same level directory as input folder\n",
    "try:\n",
    "    os.mkdir(\"Output_Video\")\n",
    "except FileExistsError:\n",
    "    print (\"Output Folder is ready already\")\n",
    "    \n",
    "# Rotate video\n",
    "angle = 90\n",
    "clip  = VideoFileClip(input_video_path)\n",
    "video_clip = clip.fl_image(lambda image: rotateImage(image, angle))\n",
    "\n",
    "# Save rotated video \n",
    "video_name_part = video_name.split(\".\")    # sample & mp4 has been split\n",
    "dest_path = os.path.join(\"Output_Video\", video_name_part[0] + \"_rotated.\" + video_name_part[1])\n",
    "%time video_clip.write_videofile(dest_path, audio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's watch it now! And reassign the input video path to our new rotated video!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassign Input path to rotated video\n",
    "input_video_path = dest_path\n",
    "\n",
    "# Watch Video                                  \n",
    "HTML(\"\"\"\n",
    "    <video width=\"960\" height=\"540\" controls>\n",
    "      <source src=\"{0}\">\n",
    "    </video>\n",
    "    \"\"\".format(dest_path)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Enumerate Frames of Video\n",
    "We need to enumerate the frames of our video to know what frames will be with the \"lights OFF\". Run the cells below to know which frame intervals will remain in the \"dark\". \n",
    "\n",
    "<b>Tips and Tricks:</b> Define a class to keep track of data from previous frames. This will be useful for running our VideoFileClip object later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoTracker():\n",
    "    def __init__(self):\n",
    "        self.current_frame = 0 \n",
    "    \n",
    "    def plusOneFrame(self):\n",
    "        self.current_frame = self.current_frame +1\n",
    "        \n",
    "        \n",
    "videoInfo = VideoTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addFrameNumber(image, frame_count, font_scale, color, thickness):\n",
    "    \"\"\"\"\n",
    "    Add frame number to an image.  \n",
    "    \"\"\"\n",
    "        \n",
    "    # Unpack and Define previously known data from video\n",
    "    frame_num   = videoInfo.current_frame\n",
    "    \n",
    "    # Add Number\n",
    "    h,w = image.shape[:2]\n",
    "    box_startX, box_startY, box_endX, box_endY = 0, 0, int(0.5*w), int(0.2*h) # Box vertices follows golden ratio \n",
    "    image[box_startY: box_endY, box_startX:box_endX] = 0                      # Black rectangle in left corner \n",
    "    text = \"Frame: \"+ str(frame_num) + \"/\" + str(frame_count)\n",
    "    text_startX, text_startY = 0, box_endY- int(box_endY*0.4)\n",
    "    cv2.putText(image, text, (text_startX, text_startY),cv2.FONT_HERSHEY_SIMPLEX, font_scale, color, thickness)\n",
    "    \n",
    "    # Update new data for next frame in video \n",
    "    videoInfo.plusOneFrame()\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first enumerate a random frame from our video to make sure everything looks ok. Set `font_scale`, `color`, and `thickness` to your taste to suit your video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load video\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "# Get General Info\n",
    "fps         = cap.get(cv2.CAP_PROP_FPS)      # OpenCV2 version 2 used \"CV_CAP_PROP_FPS\"\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "duration    = int(frame_count/fps)\n",
    "width       = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))  # float\n",
    "height      = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) # float\n",
    "\n",
    "# Read frame\n",
    "videoInfo.current_frame = random.randint(0,frame_count-1)\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, videoInfo.current_frame)\n",
    "_,frame=cap.read()\n",
    "\n",
    "# Add frame number\n",
    "font_scale, color, thickness= 2.5, (0, 255, 0), 10\n",
    "frame = addFrameNumber(frame, frame_count, font_scale, color, thickness)\n",
    "\n",
    "# Display\n",
    "print('Frames Per Sec FPS = ' + str(fps))\n",
    "print('Number of Frames   = ' + str(frame_count))\n",
    "print('Duration [Sec]     = ' + str(duration))\n",
    "minutes = int(duration/60)\n",
    "seconds = duration%60\n",
    "print('Duration [Min:Sec] = ' + str(minutes) + ':' + str(seconds))\n",
    "frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(frame)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works! So let's implement it in the whole video and place it in the Output_Video folder which we will create. <b>This may take a while...</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Create folder to place output videos at same level directory as input folder\n",
    "try:\n",
    "    os.mkdir(\"Output_Video\")\n",
    "except FileExistsError:\n",
    "    print (\"Output Folder is ready already\")\n",
    "\n",
    "# Add Frame Numbers to Video\n",
    "font_scale, color, thickness = 2.5, (0, 255, 0), 10\n",
    "videoInfo = VideoTracker()           # reset to begining\n",
    "clip  = VideoFileClip(input_video_path)\n",
    "video_clip = clip.fl_image(lambda image: addFrameNumber(image, frame_count, font_scale, color, thickness))\n",
    "\n",
    "# Save Enumerated video \n",
    "video_name_part = video_name.split(\".\")    # sample & mp4 has been split\n",
    "dest_path = os.path.join(\"Output_Video\", video_name_part[0] + \"_enumerated.\" + video_name_part[1])\n",
    "%time video_clip.write_videofile(dest_path, audio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's watch it now! Take note of the frame intervals you want to turn OFF the lights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch Video \n",
    "dest_path = os.path.join(\"Output_Video\", video_name_part[0] + \"_enumerated.\" + video_name_part[1])\n",
    "HTML(\"\"\"\n",
    "    <video width=\"960\" height=\"540\" controls>\n",
    "      <source src=\"{0}\">\n",
    "    </video>\n",
    "    \"\"\".format(dest_path)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## HED Image Processing: \"Turn OFF The Lights\":\n",
    "\n",
    "From the previous video we know now that frame intervals [93,184] , [465,1458], and [1588,1662] must be in the \"dark.\" Let's explore on a random frame from our video our HED image processing option first.\n",
    "\n",
    "For that, a neural network for edge detection called Hollistically-Nested Edge Detector (HED) will be used. This can be implemented with GPU but for simplicity we will use the OpenCV approach which uses CPU only. For more, check my list of edge detection techniques in my repo [EdgeDetection-and-ColorSpaces](https://github.com/laygond/EdgeDetection-and-ColorSpaces.git).\n",
    "\n",
    "Start by loading Model and adding extra layer to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new Layer to add to current net\n",
    "class CropLayer(object):\n",
    "    def __init__(self, params, blobs):\n",
    "        # initialize our starting and ending (x, y)-coordinates of\n",
    "        # the crop\n",
    "        self.startX = 0\n",
    "        self.startY = 0\n",
    "        self.endX = 0\n",
    "        self.endY = 0\n",
    "\n",
    "    def getMemoryShapes(self, inputs):\n",
    "        # the crop layer will receive two inputs -- we need to crop\n",
    "        # the first input blob to match the shape of the second one,\n",
    "        # keeping the batch size and number of channels\n",
    "        (inputShape, targetShape) = (inputs[0], inputs[1])\n",
    "        (batchSize, numChannels) = (inputShape[0], inputShape[1])\n",
    "        (H, W) = (targetShape[2], targetShape[3])\n",
    "\n",
    "        # compute the starting and ending crop coordinates\n",
    "        self.startX = int((inputShape[3] - targetShape[3]) / 2)\n",
    "        self.startY = int((inputShape[2] - targetShape[2]) / 2)\n",
    "        self.endX = self.startX + W\n",
    "        self.endY = self.startY + H\n",
    "\n",
    "        # return the shape of the volume (we'll perform the actual\n",
    "        # crop during the forward pass\n",
    "        return [[batchSize, numChannels, H, W]]\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # use the derived (x, y)-coordinates to perform the crop\n",
    "        return [inputs[0][:, :, self.startY:self.endY,\n",
    "                self.startX:self.endX]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our pipeline workflow through which each image will be passed in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hed_pipeline(image):\n",
    "    \n",
    "    # load model\n",
    "    print(\"[INFO] loading edge detector...\")\n",
    "    protoPath = os.path.join(\"hed_model\",\"deploy.prototxt\")\n",
    "    modelPath = os.path.join(\"hed_model\",\"hed_pretrained_bsds.caffemodel\")\n",
    "    net = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
    "\n",
    "    # register our new layer with the model\n",
    "    cv2.dnn_registerLayer(\"Crop\", CropLayer)\n",
    "\n",
    "    # Preprocess image\n",
    "    (height, width) = image.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(image, scalefactor=1.0, size=(width, height),\n",
    "                                mean=(104.00698793, 116.66876762, 122.67891434),\n",
    "                                swapRB=False, crop=False)\n",
    "\n",
    "    # Run Model: Perform a forward pass of blob image\n",
    "    print(\"[INFO] performing holistically-nested edge detection...\")\n",
    "    net.setInput(blob)\n",
    "    hed = net.forward()\n",
    "    hed = cv2.resize(hed[0, 0], (width, height))\n",
    "    hed = (255 * hed).astype(\"uint8\")\n",
    "    \n",
    "    return hed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run image through our model pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading edge detector...\n",
      "[INFO] performing holistically-nested edge detection...\n"
     ]
    }
   ],
   "source": [
    "# Load video\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Read frame\n",
    "frame_num = random.randint(0,frame_count-1)\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "_,frame=cap.read()\n",
    "\n",
    "# Apply HED to frame\n",
    "frame = hed_pipeline(frame)\n",
    "\n",
    "# Display\n",
    "frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(frame)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
